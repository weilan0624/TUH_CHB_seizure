{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "#from imblearn.over_sampling import RandomOverSampler\n",
    "from collections import Counter\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "from sklearn.utils import resample\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import io\n",
    "import os\n",
    "import struct\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import balanced_accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from scipy import signal\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.signal as signal\n",
    "from scipy import fftpack\n",
    "import math\n",
    "from matplotlib import patches\n",
    "import matplotlib.ticker as mtick"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pickle(file_dir):   \n",
    "    pickle_L=[]   \n",
    "    pickle_name=[]\n",
    "    for dirpath, dirnames, filenames in os.walk(file_dir):  \n",
    "        for file in filenames :  \n",
    "            if os.path.splitext(file)[1] == '.pickle':  \n",
    "                pickle_L.append(os.path.join(dirpath, file))  \n",
    "                pickle_name.append(os.path.join(file))  \n",
    "            \n",
    "    return pickle_L \n",
    "    #return  pickle_name\n",
    "    #return name    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pickleLoader(pklFile):\n",
    "    try:\n",
    "        while True:\n",
    "            yield pickle.load(pklFile)\n",
    "    except EOFError:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tra_path='E:/TUH_data/TUH/TUH_channel10/Children/TUH/train/'\n",
    "#tra_path='C:/Users/10096/OneDrive/Desktop/infant_pickle/train_data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_file=read_pickle(tra_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_frame=[]\n",
    "for i in range(0,len(train_file)):\n",
    "    with open(train_file[i],'rb') as file:\n",
    "        #print(file)\n",
    "        for event in pickleLoader(file):\n",
    "            df_new=event[['target',\n",
    "\n",
    "'mean_FP1_F3','FP1_F3_mobility','FP1_F3_complexity','TKEO_FP1_F3','FP1_F3_P_theta','FP1_F3_P_alpha',\n",
    "'FP1_F3_P_beta','FP1_F3_P_gamma','FP1_F3_P_total','FP1_F3_rel_theta','FP1_F3_rel_alpha',\n",
    "'FP1_F3_rel_beta','FP1_F3_rel_gamma','FP1_F3_sum_bg','std_FP1_F3','FP1_F3_pfd','FP1_F3_skew',\n",
    "'FP1_F3_kurtosis','FP1_F3_var','FP1_F3_envelope',\n",
    "\n",
    "\n",
    "'mean_F3_C3','F3_C3_mobility','F3_C3_complexity','TKEO_F3_C3','F3_C3_P_theta','F3_C3_P_alpha',\n",
    "'F3_C3_P_beta','F3_C3_P_gamma','F3_C3_P_total','F3_C3_rel_theta','F3_C3_rel_alpha',\n",
    "'F3_C3_rel_beta','F3_C3_rel_gamma','F3_C3_sum_bg','std_F3_C3','F3_C3_pfd','F3_C3_skew',\n",
    "'F3_C3_kurtosis','F3_C3_var','F3_C3_envelope',\n",
    "\n",
    "\n",
    "'mean_FP1_F7','FP1_F7_mobility','FP1_F7_complexity','TKEO_FP1_F7','FP1_F7_P_theta','FP1_F7_P_alpha',\n",
    "'FP1_F7_P_beta','FP1_F7_P_gamma','FP1_F7_P_total','FP1_F7_rel_theta','FP1_F7_rel_alpha',\n",
    "'FP1_F7_rel_beta','FP1_F7_rel_gamma','FP1_F7_sum_bg','std_FP1_F7','FP1_F7_pfd','FP1_F7_skew',\n",
    "'FP1_F7_kurtosis','FP1_F7_var','FP1_F7_envelope',\n",
    "\n",
    "\n",
    "'mean_P3_O1','P3_O1_mobility','P3_O1_complexity','TKEO_P3_O1','P3_O1_P_theta','P3_O1_P_alpha',\n",
    "'P3_O1_P_beta','P3_O1_P_gamma','P3_O1_P_total','P3_O1_rel_theta','P3_O1_rel_alpha',\n",
    "'P3_O1_rel_beta','P3_O1_rel_gamma','P3_O1_sum_bg','std_P3_O1','P3_O1_pfd','P3_O1_skew',\n",
    "'P3_O1_kurtosis','P3_O1_var','P3_O1_envelope',\n",
    "\n",
    "\n",
    "'mean_C3_P3','C3_P3_mobility','C3_P3_complexity','TKEO_C3_P3','C3_P3_P_theta','C3_P3_P_alpha',\n",
    "'C3_P3_P_beta','C3_P3_P_gamma','C3_P3_P_total','C3_P3_rel_theta','C3_P3_rel_alpha',\n",
    "'C3_P3_rel_beta','C3_P3_rel_gamma','C3_P3_sum_bg','std_C3_P3','C3_P3_pfd','C3_P3_skew',\n",
    "'C3_P3_kurtosis','C3_P3_var','C3_P3_envelope',\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "]]\n",
    "            train_frame.append(df_new)\n",
    "df_train = pd.concat(train_frame, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_train.target\n",
    "X = df_train.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming you have a feature matrix X and corresponding target variable y\n",
    "\n",
    "train_X, val_X, train_y, val_y = train_test_split(X, y, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Before OverSampling, counts of label '1': {}\".format(sum(train_y==1)))\n",
    "print(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(train_y==0)))\n",
    "\n",
    "sm = SMOTE(random_state=2)\n",
    "X_train_res, y_train_res = sm.fit_sample(train_X, train_y.ravel())\n",
    "\n",
    "print('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\n",
    "print('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n",
    "\n",
    "print(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\n",
    "print(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the lightgbm model\n",
    "import lightgbm as lgb\n",
    "classifier = lgb.LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,\n",
    "               importance_type='split', learning_rate=0.1, max_depth=-1,\n",
    "               min_child_samples=8, min_child_weight=0.001, min_split_gain=0.0,\n",
    "               n_estimators=50, n_jobs=-1, num_leaves=31, objective=None,\n",
    "               random_state=42, reg_alpha=0.0, reg_lambda=0.0, silent=True,\n",
    "               subsample=1.0, subsample_for_bin=200000, subsample_freq=0)\n",
    "classifier.fit(X_train_res, y_train_res)\n",
    "print(classifier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = classifier.predict(train_X)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(train_y, train_pred).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "sens=tp/(tp+fn)\n",
    "\n",
    "print('Spec : ',specificity)\n",
    "print('Sens : ',sens)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(train_y, train_pred))\n",
    "print(\"Balanced acc : \", balanced_accuracy_score(train_y, train_pred))\n",
    "print(\"MCC: \", matthews_corrcoef(train_y, train_pred))  \n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(train_y, train_pred))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(train_y, train_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pred = classifier.predict(X_train_res)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_train_res, train_pred).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "sens=tp/(tp+fn)\n",
    "\n",
    "print('Spec : ',specificity)\n",
    "print('Sens : ',sens)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(y_train_res, train_pred))\n",
    "print(\"Balanced acc : \", balanced_accuracy_score(y_train_res, train_pred))\n",
    "print(\"MCC: \", matthews_corrcoef(y_train_res, train_pred))  \n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(y_train_res, train_pred))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(y_train_res, train_pred,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_pred = classifier.predict(val_X)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(val_y, val_pred).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "sens=tp/(tp+fn)\n",
    "\n",
    "print('Spec : ',specificity)\n",
    "print('Sens : ',sens)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(val_y, val_pred))\n",
    "print(\"Balanced acc : \", balanced_accuracy_score(val_y, val_pred))\n",
    "print(\"MCC: \", matthews_corrcoef(val_y, val_pred))  \n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(val_y, val_pred))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(val_y, val_pred,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path='E:/TUH_data/TUH/TUH_channel10/Children/TUH/dev/'\n",
    "#tra_path='C:/Users/10096/OneDrive/Desktop/infant_pickle/train_data/'\n",
    "\n",
    "test_file=read_pickle(test_path)\n",
    "\n",
    "test_frame=[]\n",
    "for i in range(0,len(test_file)):\n",
    "    with open(test_file[i],'rb') as file:\n",
    "        #print(file)\n",
    "        for event in pickleLoader(file):\n",
    "            df_new=event[['target',\n",
    "\n",
    "'mean_FP1_F3','FP1_F3_mobility','FP1_F3_complexity','TKEO_FP1_F3','FP1_F3_P_theta','FP1_F3_P_alpha',\n",
    "'FP1_F3_P_beta','FP1_F3_P_gamma','FP1_F3_P_total','FP1_F3_rel_theta','FP1_F3_rel_alpha',\n",
    "'FP1_F3_rel_beta','FP1_F3_rel_gamma','FP1_F3_sum_bg','std_FP1_F3','FP1_F3_pfd','FP1_F3_skew',\n",
    "'FP1_F3_kurtosis','FP1_F3_var','FP1_F3_envelope',\n",
    "\n",
    "\n",
    "'mean_F3_C3','F3_C3_mobility','F3_C3_complexity','TKEO_F3_C3','F3_C3_P_theta','F3_C3_P_alpha',\n",
    "'F3_C3_P_beta','F3_C3_P_gamma','F3_C3_P_total','F3_C3_rel_theta','F3_C3_rel_alpha',\n",
    "'F3_C3_rel_beta','F3_C3_rel_gamma','F3_C3_sum_bg','std_F3_C3','F3_C3_pfd','F3_C3_skew',\n",
    "'F3_C3_kurtosis','F3_C3_var','F3_C3_envelope',\n",
    "\n",
    "\n",
    "'mean_FP1_F7','FP1_F7_mobility','FP1_F7_complexity','TKEO_FP1_F7','FP1_F7_P_theta','FP1_F7_P_alpha',\n",
    "'FP1_F7_P_beta','FP1_F7_P_gamma','FP1_F7_P_total','FP1_F7_rel_theta','FP1_F7_rel_alpha',\n",
    "'FP1_F7_rel_beta','FP1_F7_rel_gamma','FP1_F7_sum_bg','std_FP1_F7','FP1_F7_pfd','FP1_F7_skew',\n",
    "'FP1_F7_kurtosis','FP1_F7_var','FP1_F7_envelope',\n",
    "\n",
    "\n",
    "'mean_P3_O1','P3_O1_mobility','P3_O1_complexity','TKEO_P3_O1','P3_O1_P_theta','P3_O1_P_alpha',\n",
    "'P3_O1_P_beta','P3_O1_P_gamma','P3_O1_P_total','P3_O1_rel_theta','P3_O1_rel_alpha',\n",
    "'P3_O1_rel_beta','P3_O1_rel_gamma','P3_O1_sum_bg','std_P3_O1','P3_O1_pfd','P3_O1_skew',\n",
    "'P3_O1_kurtosis','P3_O1_var','P3_O1_envelope',\n",
    "\n",
    "\n",
    "'mean_C3_P3','C3_P3_mobility','C3_P3_complexity','TKEO_C3_P3','C3_P3_P_theta','C3_P3_P_alpha',\n",
    "'C3_P3_P_beta','C3_P3_P_gamma','C3_P3_P_total','C3_P3_rel_theta','C3_P3_rel_alpha',\n",
    "'C3_P3_rel_beta','C3_P3_rel_gamma','C3_P3_sum_bg','std_C3_P3','C3_P3_pfd','C3_P3_skew',\n",
    "'C3_P3_kurtosis','C3_P3_var','C3_P3_envelope',\n",
    "\n",
    "\n",
    "]]\n",
    "            test_frame.append(df_new)\n",
    "df_test = pd.concat(test_frame, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_y = df_test.target\n",
    "test_X = df_test.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = classifier.predict(test_X)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, test_pred).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "sens=tp/(tp+fn)\n",
    "\n",
    "print('Spec : ',specificity)\n",
    "print('Sens : ',sens)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_y, test_pred))\n",
    "print(\"Balanced acc : \", balanced_accuracy_score(test_y, test_pred))\n",
    "print(\"MCC: \", matthews_corrcoef(test_y, test_pred))  \n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(test_y, test_pred))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(test_y, test_pred,digits=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CHB-MIT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_path='E:/TUH_data/TUH/TUH_channel10/Children/seizure/'\n",
    "#tra_path='C:/Users/10096/OneDrive/Desktop/infant_pickle/train_data/'\n",
    "test_file=read_pickle(test_path)\n",
    "\n",
    "test_frame=[]\n",
    "for i in range(0,len(test_file)):\n",
    "    with open(test_file[i],'rb') as file:\n",
    "        #print(file)\n",
    "        for event in pickleLoader(file):\n",
    "            df_new=event[['target',\n",
    "\n",
    "'mean_FP1_F3','FP1_F3_mobility','FP1_F3_complexity','TKEO_FP1_F3','FP1_F3_P_theta','FP1_F3_P_alpha',\n",
    "'FP1_F3_P_beta','FP1_F3_P_gamma','FP1_F3_P_total','FP1_F3_rel_theta','FP1_F3_rel_alpha',\n",
    "'FP1_F3_rel_beta','FP1_F3_rel_gamma','FP1_F3_sum_bg','std_FP1_F3','FP1_F3_pfd','FP1_F3_skew',\n",
    "'FP1_F3_kurtosis','FP1_F3_var','FP1_F3_envelope',\n",
    "\n",
    "\n",
    "'mean_F3_C3','F3_C3_mobility','F3_C3_complexity','TKEO_F3_C3','F3_C3_P_theta','F3_C3_P_alpha',\n",
    "'F3_C3_P_beta','F3_C3_P_gamma','F3_C3_P_total','F3_C3_rel_theta','F3_C3_rel_alpha',\n",
    "'F3_C3_rel_beta','F3_C3_rel_gamma','F3_C3_sum_bg','std_F3_C3','F3_C3_pfd','F3_C3_skew',\n",
    "'F3_C3_kurtosis','F3_C3_var','F3_C3_envelope',\n",
    "\n",
    "\n",
    "'mean_FP1_F7','FP1_F7_mobility','FP1_F7_complexity','TKEO_FP1_F7','FP1_F7_P_theta','FP1_F7_P_alpha',\n",
    "'FP1_F7_P_beta','FP1_F7_P_gamma','FP1_F7_P_total','FP1_F7_rel_theta','FP1_F7_rel_alpha',\n",
    "'FP1_F7_rel_beta','FP1_F7_rel_gamma','FP1_F7_sum_bg','std_FP1_F7','FP1_F7_pfd','FP1_F7_skew',\n",
    "'FP1_F7_kurtosis','FP1_F7_var','FP1_F7_envelope',\n",
    "\n",
    "\n",
    "'mean_P3_O1','P3_O1_mobility','P3_O1_complexity','TKEO_P3_O1','P3_O1_P_theta','P3_O1_P_alpha',\n",
    "'P3_O1_P_beta','P3_O1_P_gamma','P3_O1_P_total','P3_O1_rel_theta','P3_O1_rel_alpha',\n",
    "'P3_O1_rel_beta','P3_O1_rel_gamma','P3_O1_sum_bg','std_P3_O1','P3_O1_pfd','P3_O1_skew',\n",
    "'P3_O1_kurtosis','P3_O1_var','P3_O1_envelope',\n",
    "\n",
    "\n",
    "'mean_C3_P3','C3_P3_mobility','C3_P3_complexity','TKEO_C3_P3','C3_P3_P_theta','C3_P3_P_alpha',\n",
    "'C3_P3_P_beta','C3_P3_P_gamma','C3_P3_P_total','C3_P3_rel_theta','C3_P3_rel_alpha',\n",
    "'C3_P3_rel_beta','C3_P3_rel_gamma','C3_P3_sum_bg','std_C3_P3','C3_P3_pfd','C3_P3_skew',\n",
    "'C3_P3_kurtosis','C3_P3_var','C3_P3_envelope',\n",
    "\n",
    "\n",
    "]]\n",
    "            test_frame.append(df_new)\n",
    "df_test = pd.concat(test_frame, ignore_index=True)\n",
    "\n",
    "test_y = df_test.target\n",
    "test_X = df_test.drop('target', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = classifier.predict(test_X)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(test_y, test_pred).ravel()\n",
    "specificity = tn / (tn+fp)\n",
    "sens=tp/(tp+fn)\n",
    "\n",
    "print('Spec : ',specificity)\n",
    "print('Sens : ',sens)\n",
    "print(\"Accuracy: \", metrics.accuracy_score(test_y, test_pred))\n",
    "print(\"Balanced acc : \", balanced_accuracy_score(test_y, test_pred))\n",
    "print(\"MCC: \", matthews_corrcoef(test_y, test_pred))  \n",
    "print(\"Confusion matrix: \\n\", metrics.confusion_matrix(test_y, test_pred))\n",
    "print(\"Classification report:\\n \", metrics.classification_report(test_y, test_pred,digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
